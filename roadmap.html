<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Roadmap &mdash; PRESC 0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Machine Learning Classifier Copies" href="ml_copies.html" />
    <link rel="prev" title="Evaluations" href="evaluations.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/classifier-bias-PRESC_v3_black_margin3.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluations.html">Evaluations</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Roadmap</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#misclassifications">Misclassifications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#conditional-metrics">Conditional metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#conditional-feature-distributions">Conditional feature distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#counterfactual-accuracy">Counterfactual accuracy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#class-fit">Class fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spatial-distributions">Spatial distributions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#robustness-to-unseen-data">Robustness to unseen data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#feature-based-splitting">Feature-based splitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#entropy-based-splitting">Entropy-based splitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#label-flipping">Label flipping</a></li>
<li class="toctree-l3"><a class="reference internal" href="#novelty">Novelty</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#stability-of-methodology">Stability of methodology</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#train-test-splitting">Train-test splitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cross-validation-folds">Cross-validation folds</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ml_copies.html">ML Classifier Copies</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">presc</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Datasets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Classification datasets</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PRESC</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Roadmap</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/roadmap.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="roadmap">
<h1>Roadmap<a class="headerlink" href="#roadmap" title="Permalink to this headline"></a></h1>
<p>This is an overview of the evaluations planned for integration into PRESC.
It is intended to give a high-level description of how these will work and
sample use cases.
Prioritization and implementation details are maintained in the repo
<a class="reference external" href="https://github.com/mozilla/PRESC/issues">issues</a>.</p>
<p>At the core of PRESC is a collection of evaluations that can be run on a given
statistical model and dataset pair to inform the developer on different
aspects of the model’s performance and behaviour.
The two main intended uses are a graphical presentation in a report and the
detection of potential issues by comparing the results against threshold
values determined by the user.
In either case, results will require some degree of interpretation in the
context of the problem domain, and it will be up to the user to decide on a
course of action to correct deficiencies in the model surfaced by these
evaluations.</p>
<p>Planned evaluations are described below, grouped by theme.
Some of these will lend themselves to multiple possible visualizations or
summaries, while others will be applicable in a single clear way.
The first step in developing many of these will be to build a prototype and test
them out against different models and datasets to get an idea of how they
behave.
Related literature or implementations that we are aware of are referenced below.
Contributions that link additional references to related work are welcome.</p>
<p>For each one, we list expected inputs and output structure, as well as the ways
we expect it to be used.
The description focuses on the underlying computation rather than the ways
results should be presented or visualized.
For some of these, we will want to further summarize the outputs, while others
will be reported as is.</p>
<div class="section" id="misclassifications">
<h2>Misclassifications<a class="headerlink" href="#misclassifications" title="Permalink to this headline"></a></h2>
<p>Many common accuracy metrics involve scalar counts or rates computed from the
confusion matrix.
However, the misclassified points themselves carry much more information about
the model behaviour.
They are indicative of failures in the model, and understanding why they were
misclassified can help improve it.</p>
<p>For example:</p>
<ul class="simple">
<li><p>Is the misclassification due to the characteristics of the point itself or to
the model?</p>
<ul>
<li><p>It may not be surprising for an outlier to get misclassified by most
reasonable models.</p></li>
<li><p>A point in an area of high overlap between the classes may get
misclassified by some candidate models and not by others, depending on
where the decision boundary lands.</p></li>
</ul>
</li>
<li><p>How different is the distribution of misclassified points in feature space
from that for correctly classified points?</p>
<ul>
<li><p>Is there evidence of systematic bias?</p></li>
</ul>
</li>
</ul>
<p><strong>Application scope:</strong> These generally apply to the predictions on a test set by
a trained model, such as the final evaluation on a held-out test set or model
selection on a validation set.</p>
<div class="section" id="conditional-metrics">
<h3>Conditional metrics<a class="headerlink" href="#conditional-metrics" title="Permalink to this headline"></a></h3>
<p>This is implemented in the
<a class="reference external" href="https://github.com/mozilla/PRESC/blob/master/presc/evaluations/conditional_metric.py">conditional_metric</a>
module.</p>
<p>Standard performance metrics such as accuracy, precision and recall are
computed by summmarizing overall differences between predicted and true labels.
PRESC will additionally compute these differences restricted to subsets of the
feature space or test set.
This way, the confusion matrix and related metrics can be viewed as they vary
across the values of a feature.
This is similar to calibration, which considers accuracy as a function of
predicted score.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Predicted labels for a test set from a trained model</p></li>
<li><p>Scheme for partitioning the test set</p>
<ul>
<li><p>eg. binning values of a given feature</p></li>
</ul>
</li>
<li><p>Metric</p>
<ul>
<li><p>function of predicted and true labels</p></li>
</ul>
</li>
</ul>
<p><strong>Output:</strong> Metric values for each partition</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Performance metrics as a function of partitions:</p>
<ul>
<li><p>Misclassification counts by class</p></li>
<li><p>Standard accuracy metrics (eg. accuracy, precision, recall)</p></li>
<li><p>Proportion of misclassified belonging to a specific class</p></li>
</ul>
</li>
<li><p>Deviation of these per-partition values from the value over the entire test
set</p></li>
</ul>
<p><strong>Type</strong>: Model performance metric</p>
</div>
<div class="section" id="conditional-feature-distributions">
<h3>Conditional feature distributions<a class="headerlink" href="#conditional-feature-distributions" title="Permalink to this headline"></a></h3>
<p>This is implemented in the
<a class="reference external" href="https://github.com/mozilla/PRESC/blob/master/presc/evaluations/conditional_distribution.py">conditional_distribution</a>
module.</p>
<p>In a sense this reverses the conditioning of the conditional confusion matrix.
We compute the distribution of a feature over the test set restricted to each
cell of the confusion matrix.
This allows us to compare distributions between misclassified and correctly
classified points.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Predicted labels for a test set from a trained model</p></li>
<li><p>Column of data from the test set</p>
<ul>
<li><p>eg. values of a feature</p></li>
<li><p>could also be predicted scores or a function of the features</p></li>
</ul>
</li>
</ul>
<p><strong>Output:</strong> Distributional representation (eg. value counts, histogram or
density estimate) for each cell in the confusion matrix</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Feature exploration conditional on test set predicted outcomes</p></li>
<li><p>Assessment of differences between misclassified and correctly classified
points in terms of their distribution in feature space</p>
<ul>
<li><p>Within one class, between multiple classes, or relative to the training
set</p></li>
<li><p>Evidence of bias in the misclassifications</p></li>
<li><p>Are misclassifications concentrated in an area of strong overlap between
the classes in the training set?</p></li>
<li><p>Are misclassifications clustered, eg. separated from the majority of
training points of that class?</p></li>
</ul>
</li>
</ul>
<p><strong>Type</strong>: Feature distributions</p>
</div>
<div class="section" id="counterfactual-accuracy">
<h3>Counterfactual accuracy<a class="headerlink" href="#counterfactual-accuracy" title="Permalink to this headline"></a></h3>
<p>How much does the performance of an optimal model which correctly classifies a
misclassified point differ from the current model?
This is measured by searching (the parameter space) for the best performing
model, subject to the constraint that a specific point is correctly classified
(<a class="reference external" href="../literature/ML_IRL_20_CFA.pdf">Bhatt et al (2020)</a>).</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Trainable model</p>
<ul>
<li><p>ie. model specification and training set</p></li>
</ul>
</li>
<li><p>Labeled sample point</p>
<ul>
<li><p>ie. misclassified test set point</p></li>
</ul>
</li>
</ul>
<p><strong>Output:</strong> Trained model which correctly classifies the sample point</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Cost measure for correcting misclassifications</p></li>
<li><p>Measure of whether a misclassification is more likely due to its inherent
characteristics or to the choice of model.</p>
<ul>
<li><p>If forcing a correct classification substantially decreases accuracy, then
it is likely an unusual point relative to the training set (ie. an
influential point in the statistical sense).</p></li>
<li><p>If the change in accuracy is minimal, the misclassification may be an
artifact of the methodology used to select the model.</p></li>
</ul>
</li>
</ul>
<p><strong>Type</strong>: Per-sample metric applied to misclassifications</p>
</div>
<div class="section" id="class-fit">
<h3>Class fit<a class="headerlink" href="#class-fit" title="Permalink to this headline"></a></h3>
<p>In addition to looking at distributions across misclassifications, it is useful
to have a distributional goodness-of-fit metric for how much a misclassified
point “belongs” to each class.
We compute entropy-based goodness-of-fit between a misclassified point and each
“true” class as represented by the training set.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Sample point</p>
<ul>
<li><p>ie. misclassified test set point</p></li>
</ul>
</li>
<li><p>Dataset</p>
<ul>
<li><p>ie. training set</p></li>
</ul>
</li>
</ul>
<p>Datapoints can refer to either the original feature space or an
embedding/transformation.</p>
<p><strong>Output:</strong> Scalar goodness-of-fit measure for each class</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Measure of surprisal for misclassifications</p>
<ul>
<li><p>Was the point misclassified because it looks much more like a member of
its predicted class than its true class?</p></li>
<li><p>If it fits well in multiple classes, it may be in an area of high overlap</p></li>
<li><p>If it doesn’t fit well in any class, it may be an outlier.</p></li>
</ul>
</li>
<li><p>Deviation from a baseline distribution computed using the same approach for
correctly classified points</p></li>
</ul>
<p><strong>Type</strong>: Per-sample metric applied to misclassifications</p>
</div>
<div class="section" id="spatial-distributions">
<h3>Spatial distributions<a class="headerlink" href="#spatial-distributions" title="Permalink to this headline"></a></h3>
<p>This is partially implemented in the
<a class="reference external" href="https://github.com/mozilla/PRESC/blob/master/presc/evaluations/spatial_distribution.py">spatial_distribution</a>
module.</p>
<p>In some cases, it will be helpful to understand where a misclassified point lies
in the feature space in relation to other training points.
While this does not translate to intuition about model behaviour for all types
of model, it can still be useful as a view into the geometry of the feature
space.
PRESC does this by computing the distribution of pairwise distances between a
misclassified point and other training points split by class.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Sample point</p>
<ul>
<li><p>ie. misclassified test set point</p></li>
</ul>
</li>
<li><p>Dataset</p>
<ul>
<li><p>ie. training set</p></li>
</ul>
</li>
<li><p>Metric to measure distances in the feature space</p>
<ul>
<li><p>eg. Euclidian</p></li>
</ul>
</li>
</ul>
<p>Datapoints and metric can refer to either the original feature space or an
embedding/transformation.</p>
<p><strong>Output:</strong> Distributional representation (histogram or density estimate) for
each class</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Geometric class-fit measure for misclassifications</p>
<ul>
<li><p>Can help to distinguish between misclassifications that are outliers (far
from all training points), those which lie in an area of high overlap, and
those which are closer to a different class</p></li>
</ul>
</li>
<li><p>Deviation from a baseline distribution computed using the same approach for
correctly classified points</p></li>
</ul>
<p><strong>Type</strong>: Per-sample metric applied to misclassifications</p>
</div>
</div>
<div class="section" id="robustness-to-unseen-data">
<h2>Robustness to unseen data<a class="headerlink" href="#robustness-to-unseen-data" title="Permalink to this headline"></a></h2>
<p>Ideally, our model should perform well for unseen data points for which
predictions are requested.
Standard performance measures, computed by averaging results over a random split
of the data, represent average-case performance for data which is identically
distributed to the training data, an assumption which is often not met in
practice.
Here we consider performance evaluations that account for distributional
differences in training and test sets.</p>
<p><strong>Application scope:</strong> These can be applied either as strategies for model
selection or as an evaluation methodology on a final test set.</p>
<div class="section" id="feature-based-splitting">
<h3>Feature-based splitting<a class="headerlink" href="#feature-based-splitting" title="Permalink to this headline"></a></h3>
<p>While we don’t know along which dimensions the unseen data will differ from our
training set, we can take into account possible training set bias by validating
over explicitly biased splits.
These are generated by holding out training points whose values for a given
feature fall in a particular range.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Dataset</p>
<ul>
<li><p>ie. training set</p></li>
</ul>
</li>
<li><p>Scheme for partitioning the test set</p>
<ul>
<li><p>eg. binning values of a given feature</p></li>
</ul>
</li>
</ul>
<p>Datapoints and partitioning can refer to either the original feature space or an
embedding/transformation.</p>
<p><strong>Output:</strong> Sequence of splits of the dataset holding out one partition each time</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Model selection using cross-validation taking training set bias into account</p></li>
<li><p>Feature selection using susceptibility to bias as a criterion</p></li>
<li><p>Model performance range estimate in the face of biased data</p>
<ul>
<li><p>ie. evaluate on test-set data belonging to the held out partition, having
trained on training data in the other partitions</p></li>
</ul>
</li>
<li><p>Deviation from overall performance metric over the entire test set</p></li>
</ul>
<p><strong>Type</strong>: Dataset splitting scheme for validation</p>
</div>
<div class="section" id="entropy-based-splitting">
<h3>Entropy-based splitting<a class="headerlink" href="#entropy-based-splitting" title="Permalink to this headline"></a></h3>
<p>Another approach to non-random splitting is to partition in terms of
distributional differences rather than the values of a specific feature.
Here we generate splits achieving a target distributional dissimilarity value
between the train and test portions.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Dataset</p>
<ul>
<li><p>ie. training set</p></li>
</ul>
</li>
</ul>
<p>Datapoints can refer to either the original feature space or an
embedding/transformation.</p>
<p><strong>Output:</strong> Sequence of randomized splits of the dataset achieving target dissimilarity (K-L divergence) values</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Model selection using cross-validation taking into account robustness to
unseen data</p></li>
<li><p>Model performance range estimate in the face of data shift</p>
<ul>
<li><p>ie. select one subset from the training set and another from the test set</p></li>
</ul>
</li>
<li><p>Deviation from overall performance metric over the entire test set</p></li>
</ul>
<p><strong>Type</strong>: Dataset splitting scheme for validation</p>
</div>
<div class="section" id="label-flipping">
<h3>Label flipping<a class="headerlink" href="#label-flipping" title="Permalink to this headline"></a></h3>
<p>How many training labels would have to change for the decision boundary to move?
A more robust model which is not overfit should be able to sustain more label
changes without a significant impact on its performance.
Label flipping could occur in practice, for example, if some of the training
data is mislabeled, or certain areas of the feature space can reasonably belong
to multiple classes.
To measure this, we compute the change in model performance as more and more
training point labels are flipped.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Training set</p></li>
<li><p>Model specification</p></li>
<li><p>Performance measurement scheme</p>
<ul>
<li><p>ie. test set and performance metric</p></li>
</ul>
</li>
</ul>
<p><strong>Output:</strong> Function (sequence of tuples) mapping number of flipped labels to
performance metric values</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Measure of robustness to mislabeled data</p></li>
<li><p>Measure of overfitting</p></li>
<li><p>Influence measure for points or clusters in the training data</p></li>
</ul>
<p><strong>Type</strong>: Model performance metric</p>
</div>
<div class="section" id="novelty">
<h3>Novelty<a class="headerlink" href="#novelty" title="Permalink to this headline"></a></h3>
<p>Scenarios in which classification models are deployed generally evolve over
time, and eventually the data used to train the model may no longer be
representative of the cases for which predictions are requested.
PRESC will include functionality to determine how much a new set of labeled
data (if available) has diverged from the current training set.
This will help to inform when a model update is needed.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Previous dataset</p>
<ul>
<li><p>ie. current training set</p></li>
</ul>
</li>
<li><p>New dataset</p>
<ul>
<li><p>ie. newly available labeled data</p></li>
</ul>
</li>
</ul>
<p><strong>Output:</strong> Similarity measure between the two datasets (scalar or
distributional)</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Measure of novelty for new labeled data (eg. available as a result of
ongoing human review)</p></li>
<li><p>Measure of appropriateness of the model on new data</p>
<ul>
<li><p>eg. improvement in performance on the new data between a model trained
including the new data and the original model, as a function of novelty</p></li>
</ul>
</li>
<li><p>Decision rule for when to trigger a model update</p></li>
<li><p>Deviation from baseline computed from subsets of the existing training set</p></li>
</ul>
<p><strong>Type</strong>: Dataset comparison metric</p>
</div>
</div>
<div class="section" id="stability-of-methodology">
<h2>Stability of methodology<a class="headerlink" href="#stability-of-methodology" title="Permalink to this headline"></a></h2>
<p>While models are generally selected to maximize some measure of performance,
the final choice of model also carries an inherent dependence on the
methodology used to train it.
For example, if a different train-test split were used, the final model would
likely be slightly different.
These analyses measure the effects of methodological choices on model
performance, with the goal of minimizing them.
Note that, for any of these approaches which uses resampling to estimate
variability, computation cost needs to be taken into account as a part of the
design.</p>
<p><strong>Application scope:</strong> These generally require a model specification and
training set, and can be applied post-hoc (to assess error in reported
results) or prior to training (to help select methodology, using an assumed
prior model choice).</p>
<div class="section" id="train-test-splitting">
<h3>Train-test splitting<a class="headerlink" href="#train-test-splitting" title="Permalink to this headline"></a></h3>
<p>This is implemented in the
<a class="reference external" href="https://github.com/mozilla/PRESC/blob/master/presc/evaluations/train_test_splits.py">train_test_splits</a>
module.</p>
<p>When training a model, a test set is typically held out at the beginning so as
to provide an unbiased estimate of model performance on unseen data.
However, the size of this test set itself influences the quality of this
estimate.
To assess this, we consider the variability and bias in performance metrics as
the test set size varies.
This is estimated by splitting the input dataset at different proportions and
training and testing a model across these.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Dataset</p>
<ul>
<li><p>ie. training set</p></li>
</ul>
</li>
<li><p>Model specification</p></li>
<li><p>Performance metric</p></li>
</ul>
<p><strong>Output:</strong> Function (sequence of tuples) mapping train-test split size to
performance estimates represented as a mean with confidence bounds</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Measure of error (variance/bias) in test set performance evaluations</p></li>
<li><p>Selection of train-test split size to minimize bias and variance</p></li>
<li><p>Deviation from average-case performance</p></li>
</ul>
<p><strong>Type</strong>: Model performance confidence metric</p>
</div>
<div class="section" id="cross-validation-folds">
<h3>Cross-validation folds<a class="headerlink" href="#cross-validation-folds" title="Permalink to this headline"></a></h3>
<p>Similarly, the choice of validation methodology will influence the quality of
estimates obtained using it.
PRESC assesses this by computing model cross-validation (CV) model performance
estimates across different numbers of folds.</p>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Dataset</p>
<ul>
<li><p>ie. training set</p></li>
</ul>
</li>
<li><p>Model specification</p></li>
<li><p>Performance metric</p></li>
</ul>
<p><strong>Output:</strong> Function (sequence of tuples) mapping number of CV folds to
performance estimates represented as a mean with confidence bounds</p>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p>Measure of error (variance/bias) in CV performance evaluations</p></li>
<li><p>Selection of number of CV folds to minimize bias and variance</p></li>
<li><p>Deviation from average-case performance</p></li>
</ul>
<p><strong>Type</strong>: Model performance confidence metric</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="evaluations.html" class="btn btn-neutral float-left" title="Evaluations" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ml_copies.html" class="btn btn-neutral float-right" title="Machine Learning Classifier Copies" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Mozilla Foundation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>